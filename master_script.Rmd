---
title: "masterscript"
author: "William Rohde Madsen"
output: html_document
---

# Packages
```{r load-packages, echo = FALSE, include = FALSE}
library(tidyverse)
library(readxl)
library(lubridate)
library(stringr)
library(janitor)
library(rgdal) # for shapefiles
library(sf) # manipulating spatial data
library(lme4) # for glmer, etc. modelling
library(rtweet) # using Twitter API
library(httr)
library(tfse) # to save token to environment
library(ggrepel)
library(reticulate) # using python code in R
library(vroom) # loading data faster
library(ndjson)
library(quanteda)
library(tidytext)
library(rvest) # reading html websites
library(maps)
library(ggmap)
library(stars) # rasters

#source("src/theme.R")

```

# Load data
```{r load-supp}
###### Load supplementary data

# English speakers data from the UN
un_lang_raw <- read_csv("data/raw/UNdata_Export_20210203_194758725.csv")

# English speakers data pasted from Ethnologue
ethno_raw <- read_excel("data/raw/ethnologue_english.xlsx")

# English speakers data from Wikipedia
wiki_raw <- read_excel("data/raw/english_wiki.xlsx", skip = 1)

# Corruption perception index
cpi_raw <- read_excel("data/raw/CPI2020_GlobalTablesTS_210125.xlsx",
                      sheet = "CPI Timeseries 2012 - 2020",
                      skip = 2)

# WGI data
wgi_raw <- read_excel("data/raw/wgidataset.xlsx", sheet = "VoiceandAccountability", skip = 12)

# UN population estimates
## https://population.un.org/wpp/Download/Standard/CSV/
pop_raw <- read_csv("data/raw/WPP2019_TotalPopulationBySex.csv")

# GDP PPP from World Bank
## https://data.worldbank.org/indicator/NY.GDP.MKTP.PP.KD
gdp_ppp_raw <- read_csv("data/raw/API_NY.GDP.MKTP.PP.KD_DS2_en_csv_v2_1928416.csv", skip = 3)

# Twitter users by country, Hootsuite, January 2020
hootsuite_raw <- read_excel("data/raw/twitter_hootsuite.xlsx", skip = 1)


```

```{r load-get-help}
###### Load data to use in collecting Tweets

# Load leader data from REIGN
reign_raw <- read_csv("data/raw/REIGN_2021_2.csv")

# Load geocodes, INITIAL
cap_geo_raw <- read_csv("data/raw/country-capitals.csv")

# Get proxy list, rotating IPs for scraping tweets without getting blocked
## https://free-proxy-list.net/
proxy_raw <- read_html("https://free-proxy-list.net/#")

```

```{r load-subnational}
###### Load subnational data

# Load Global Data Lab data
## https://globaldatalab.org/areadata/download_files/
gdl_raw <- read_csv("data/raw/GDL-AreaData390 (1).csv")

# Subnational infant mortality
## https://sedac.ciesin.columbia.edu/data/set/povmap-global-subnational-infant-mortality-rates-v2/data-download


```

```{r load-spatial}
###### Load spatial data

# GDL shapefiles
# gdl_shp_raw <- readOGR(dsn = "data/raw/GDL Shapefiles V4 (1)/",
#                        layer = "GDL Shapefiles V4")

# save(gdl_shp_raw, file = "data/gdl_shp_raw.rdata") # save as rdata to speed up loading after first time
load("data/gdl_shp_raw.rdata")

# Africapolis
## https://africapolis.org/data
afri_polis_raw <- read_excel("data/raw/Africapolis_agglomeration_2015.xlsx", skip = 15)

# Load World Bank boundaries
wb_0_raw <- st_read("data/WB_Boundaries_GeoJSON_highres/WB_countries_Admin0.geojson")

```


```{r load-gpw}
###### Load GPW data

# Load GPW4 Admin Unit data
# gpw_raw <- list.files(pattern = "gpw_v4_admin_unit.+\\.csv", recursive = TRUE) %>%
#   map_df(~read_csv(.))


# Load GPW4 Admin Unit shapefiles
## Nigeria
nga_shp_raw <- st_read("data/gpw_admin/gpw-v4-admin-unit-center-points-population-estimates-rev11_nga_shp/gpw_v4_admin_unit_center_points_population_estimates_rev11_nga.shp")

# Load GPW4 Population count, 15 minute resolution (30 km), raster data
gpw_30_raw <- read_stars("data/gpw_pop/gpw-v4-population-count-rev11_2020_15_min_tif/gpw_v4_population_count_rev11_2020_15_min.tif")

# Natural Earth cultural country boundaries
ne_raw <- st_read("data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp")

# Data quality files from GPW
gpw_context_raw <- read_stars("data/gpw_quality//gpw-v4-data-quality-indicators-rev11_context_2pt5_min_tif/gpw_v4_data_quality_indicators_rev11_context_2pt5_min.tif")
gpw_mean_raw <- read_stars("data/gpw_quality/gpw-v4-data-quality-indicators-rev11_mean_adminunitarea_2pt5_min_tif/gpw_v4_data_quality_indicators_rev11_mean_adminunitarea_2pt5_min.tif")


```


# Format data
```{r format-supp}
###### Format and bind supplementary data
# This will serve to decide and describe the countries in focus

###### Clean
# UN language population
## Appears to only show population by primary language
## May use rural, urban distinctions for weighting
un_lang_raw %>%
  clean_names() %>%
  filter(language %in% c("Total", "English")) %>%
  mutate(value = round(value)) %>%
  pivot_wider(names_from = language, values_from = value) %>%
  clean_names() %>%
  mutate(english_prop = english/total %>% round) %>%
  rename(country = country_or_area) %>%
  group_by(country, area, sex) %>%
  filter(year == max(year))

# Ethnologue language data, English speakers
ethno <- ethno_raw %>%
  mutate(country = if_else(grepl("Hide Details", English), gsub("Hide Details", "", English), NA_character_),
         country = if_else(English == "English", "United Kingdom", country),
         name = lag(English),
  ) %>%
  fill(country, name) %>%
  filter(lag(English) %in% c("User Population", "Location", "Language Status", "Other Comments")) %>%
  pivot_wider(names_from = name, values_from = English) %>%
  clean_names() %>%
  mutate(eng_total = gsub(" .+", "", user_population) %>% gsub(",", "", .) %>% as.integer,
         l1 = if_else(grepl("L1", user_population),
                      gsub(".+ L1 users: ", "", user_population),
                      NA_character_),
         l1_src = str_extract(l1,  "(?<=\\().+?(?=\\))"),
         l1 = sub(" .+", "", l1) %>% gsub(",", "", .) %>% as.integer,
         l1_yr = str_extract(l1_src, "\\d+") %>% as.integer,
         l2 = if_else(grepl("L2", user_population),
                      gsub(".+ L2 users: ", "", user_population),
                      NA_character_),
         l2_src = str_extract(l2,  "(?<=\\().+?(?=\\))"),
         l2 = sub(" .+", "", l2) %>% gsub(",", "", .) %>% as.integer,
         l2_yr = str_extract(l2_src, "\\d+") %>% as.integer,
         total_yr = if_else(l2 > l1, l2_yr, l1_yr),
         total_yr = if_else(is.na(total_yr), l2_yr, total_yr) %>% if_else(is.na(.), l1_yr, .),
         total_src = if_else(is.na(l1_src) & is.na(l2_src),
                             str_extract(user_population,  "(?<=\\().+?(?=\\))"),
                             NA_character_),
         total_yr = if_else(is.na(total_yr),
                            str_extract(total_src, "\\d+") %>% as.integer,
                            total_yr)
  )

## Subset key variables
ethno_sub <- ethno %>%
  select(country, eng_total, total_yr)

# English speakers Wikipedia
wiki_eng <- wiki_raw %>%
  slice(-1) %>%
  clean_names() %>%
  rename(eligible_pop = eligible_population,
         total_eng_speak = total_english_speakers,
         as_1st_lang = as_first_language,
         as_additional = as_an_additional_language) %>%
  mutate(across(c(2:5), as.integer),
         eng_prop_wiki = total_eng_speak/eligible_pop,
         country = str_trim(country),
  ) %>%
  select(country, eng_prop_wiki)

# Corruption
cpi <- cpi_raw %>%
  clean_names() %>%
  rename_with(~if_else(str_count(., "_") == 2, sub("_", "", .), .)) %>% # remove first _ for those with two
  pivot_longer(c(4:ncol(.)),
               names_to = c(".value", "year"),
               names_pattern = "(.+)_(.+)") %>%
  transmute(country, year = as.double(year), cpiscore)

# WGI, Voice and accountability
## paste the first row with the column name and assigning them as column names
colnames(wgi_raw) <- paste0(colnames(wgi_raw), wgi_raw[1, ])
wgi <- wgi_raw %>%
  rename_with(~gsub("\\.\\.\\.\\d*", "_", .)) %>%
  slice(-1) %>%
  pivot_longer(cols = c(3:ncol(.)),
               names_to = c("year", "name"),
               names_pattern = "(.+)_(.+)",
               values_to = "wgi_est"
  ) %>%
  filter(name == "Estimate") %>%
  transmute(country = `_Country/Territory`,
            year = as.integer(year),
            wgi_est = if_else(wgi_est == "#NA", as.double(NA), as.double(wgi_est))
  )

# UN population estimates
pop <- pop_raw %>%
  clean_names() %>%
  rename(country = location, year = time) %>%
  mutate(across(c(7:10), ~.*1000)) %>%
  filter(variant == "Medium") %>%
  filter(year %in% c(1989:2021)) %>%
  select(country, year, pop_total)

# GDP PPP data
gdp_ppp <- gdp_ppp_raw %>%
  pivot_longer(c(5:ncol(.)), names_to = "year", values_to = "gdp_ppp") %>%
  clean_names() %>%
  filter(gdp_ppp != "X66") %>%
  transmute(country = country_name, year = as.integer(year), gdp_ppp)

# Twitter users, January 2020
hootsuite <- hootsuite_raw %>%
  transmute(country,
            twitter_users = as.numeric(users)*1000) # thousands

###### Bind
supp <- cpi %>%
  full_join(ethno_sub, by = c("country", "year" = "total_yr")) %>%
  left_join(wiki_eng, by = "country") %>%
  left_join(pop, by = c("country", "year")) %>%
  left_join(gdp_ppp, by = c("country", "year")) %>%
  left_join(wgi, by = c("country", "year")) %>%
  left_join(hootsuite, by = c("country")) %>%
  mutate(eng_prop = eng_total/pop_total,
         gdp_ppp_pc = gdp_ppp/pop_total,
         twitter_users_pc = twitter_users/pop_total
  ) %>%
  filter(!is.na(eng_prop))



```

```{r format-get-help}
###### Format and bind get help data

###### Clean
# Geocodes
cap_geo <- cap_geo_raw %>%
  clean_names() %>%
  mutate(geocode = paste0(capital_latitude, ",", capital_longitude, ",20km")) %>%
  select(country = country_name, capital = capital_name, geocode)

# Leadership from REIGN
reign <- reign_raw %>%
  clean_names() %>%
  select(country, leader, year, month) %>%
  filter(lag(leader) != leader | lead(leader) != leader) %>%
  mutate(date_type = if_else(lead(leader) != leader, "end", "start"),
         date = paste0(year, "-", str_pad(month, 2, "left", pad = "0"), "-01"),
         date = as.Date(date),
  ) %>%
  filter(!(year < 2006 & date_type == "end" | date_type == "start" & lead(year) < 2006)) %>% # drop terms before 2006 (Twitter's founding)
  select(country, leader, date_type, date) %>%
  group_by(country, leader, date_type) %>%
  mutate(term_n = paste0("term ", 1:n())) %>% # count terms per leader NEED TO ADJUST FOR NAMES, e.g. LÃ¸kke, father-son?
  ungroup() %>%
  pivot_wider(names_from = date_type, values_from = date) %>%
  mutate(start = if_else(is.na(start), end, start),
  ) %>%
  rowwise() %>%
  mutate(date = list(seq.Date(start, end, by = "day"))) %>%
  tidyr::unnest(date) %>%
  mutate(country = case_when(country == "USA" ~ "United States",
                             TRUE ~ country),
         date_plus_one = date + days(1)) %>%
  filter(date >= as.Date("2006-07-15")) # when Twitter full version went live

# Proxy IPs
# proxy <- tibble(proxy = html_nodes(proxy_raw, "#proxylisttable td:nth-child(1)") %>% html_text(trim = TRUE),
#                 port = html_nodes(proxy_raw, "#proxylisttable td:nth-child(2)") %>% html_text(trim = TRUE),
#                 https = html_nodes(proxy_raw, "#proxylisttable td:nth-child(7)") %>% html_text(trim = TRUE)
# ) %>%
#   filter(https == "yes") %>% # only https types
#   transmute(proxy,
#             port,
#             proxy_type = "http",
#             proxy_no = row_number()
#             )

proxy <- read_tsv("https://api.proxyscrape.com/v2/?request=getproxies&protocol=http&timeout=100&country=all&ssl=all&anonymity=all&simplified=true") %>%
  rename(proxy = 1) %>%
  transmute(port = gsub(".+:", "", proxy) %>% as.integer,
            proxy = gsub("\\:.*", "", proxy),
            proxy_type = "http",
            proxy_no = row_number()
  )

###### Bind all together
get_help <- reign %>%
  left_join(cap_geo, by = "country") %>%
  mutate(proxy_no = rep(1:nrow(proxy), length.out = nrow(.))) %>% # repeat 1-300 to add proxy IPs
  left_join(proxy, by = "proxy_no") %>% # add rotating proxies
  mutate(port = as.integer(port))


```

```{r format-subnational}
###### Format subnational data

# GDL data
gdl <- gdl_raw %>%
  select(country, region, level, year, popshare)

```

```{r format-spatial}
###### Format spatial data

###### GDL shapefiles
# Convert sp into sf dataframe
gdl_sf <- st_as_sf(gdl_shp_raw)

# Get centroids
gdl_centroids <- st_centroid(gdl_sf$geometry) %>%
  st_coordinates %>%
  as_tibble() %>%
  rename_with(~paste0("centroid_", .))

# Simplify and add centroids
gdl_simp <- gdl_sf %>%
  mutate(geometry = st_simplify(geometry, dTolerance = 0.05)) %>%
  bind_cols(gdl_centroids)

###### Format NE boundary data
ne <- st_as_sf(ne_raw) %>%
  clean_names() %>%
  select(admin, geometry)

###### Format GPW data

## Raster data, population count
# Export from raster to polygons
gpw_30 <- st_as_sf(gpw_30_raw, as_points = FALSE, merge = FALSE) %>%
  rename(pop = 1)

# Get and bind GPW centroids
gpw_centroids <- st_centroid(gpw_30$geometry) %>%
  st_coordinates %>%
  as_tibble() %>%
  rename_with(~paste0("centroid_", .))

gpw_30_ext <- gpw_30 %>%
  bind_cols(gpw_centroids) %>%
  st_join(., ne)

## Admin unit shapefiles
nga_shp <- st_as_sf(nga_shp_raw) %>%
  mutate(across(c(UN_2020_E), ~as.numeric(levels(.))[.])) # turn factor into numeric

# ## Data quality context
# # Convert to sf
# gpw_context <- st_as_sf(gpw_context_raw, as_points = FALSE, na.rm = FALSE)
# gpw_mean <- st_as_sf(gpw_mean_raw, as_points = FALSE, na.rm = FALSE)
# 
# plot(gpw_context_raw)
# 
# st_is_valid(gpw_context_raw)
# 
# summary(gpw_context_raw)
# 
# st_contour(gpw_mean_raw, contour_lines = TRUE) %>% plot()
# 
# gpw_mean <- st_contour(gpw_mean_raw, contour_lines = TRUE)
# st_xy2sfc(gpw_mean_raw, as_points = FALSE)


###### World Bank boundaries
wb_0_raw %>% plot()


```

```{r bind-subnat-shapefile}
###### Bind subnational data to shapefiles
subnat <- gdl_simp %>%
  left_join(gdl, by = c("region", "country"))

```


# Get Tweets
```{r get-tweets-py}
###### Get Tweets with Python's Twint

###### Set up
# python version
use_python("/usr/local/bin/python3", required = TRUE)

# Source get_tweets Python function
source_python("src/py/get_tweets.py", convert = FALSE)

###### Get tweets and save as JSONs
# Testing
# get_tweets("Mnangagwa", "en", "-17.81666667,31.033333,20km",  5, "2020-01-01", "2020-01-05",
#            "data/test.json"
#            #"77.73.241.154", as.integer(8080), "http"
#            )

# Map across help data
get_help %>%
  filter(country == "Zimbabwe") %>%
  filter(date > as.Date("2020-01-01")) %>%
  #filter(date < as.Date("2020-01-10")) %>%
  mutate(csv_date = format(date, "%Y_%m_%d"),
         date = paste(date),
         date_plus_one = paste(date_plus_one),
  ) %>%
  mutate(tweets = pmap(list(leader, geocode, date, date_plus_one, country),
                       ~get_tweets(..1,
                                   "en",
                                   ..2,
                                   20,
                                   ..3,
                                   ..4,
                                   paste0("data/tweets/", ..1, "_", ..5, "_", ..3, ".json")
                                   # ..6,
                                   # ..7,
                                   # ..8
                       )
  )
  )

```

# Load Tweets
```{r load-tweets}
###### Load all Tweets
# Circa size of Tweets
file.size("data/tweets")/1000 # mb

# Function to read in and add file name as column
read_tweets_back <- function(csv){
  
  df <- stream_in(csv) %>%
    as_tibble()
  
  df$filename <- csv
  
  df
  
}

#read_tweets_back("data/tweets/Buhari2018-01-03.json")

# Map across function load all Tweets
tweets_raw <- list.files("data/tweets/", full.names = TRUE) %>%
  map_df(~read_tweets_back(.))

```


# Format Tweets
```{r format-tweets}
###### Tidy and format tweets
tweets <- tweets_raw %>%
  # select(contains("hashtags.")) %>%
  # view()
  unite(col = "hashtags_", contains("hashtags."), na.rm = TRUE) %>%
  unite(col = "photos_", contains("photos."), na.rm = TRUE) %>%
  unite(col = "urls_", contains("urls."), na.rm = TRUE) %>%
  relocate(matches("//d|0"), .after = last_col()) %>%
  mutate(filename,
         date = as.Date(date),
         country = gsub(".*_(.*)_.*", "\\1", filename),
         leader = gsub(paste0(".*//(.*)_", country, ".*"), "\\1", filename)
  ) %>%
  clean_names() %>%
  select(date, name, country, leader, place, everything(),-c(contains("mentions"), contains("reply"), "filename"))

tweets

names(tweets)

```




# Sentiment analysis
```{r sentiment-analysis}
###### Sentiment analysis of Tweets


```

```{r extract-covariates}
###### Extract individual-level covariates


```

# Plots

```{r plot-eng-sources}
###### Plot different data on English speakers
supp %>%
  ggplot(.,
         aes(x = eng_prop_wiki,
             y = eng_prop) # ethno
  ) +
  geom_point() +
  geom_label_repel(aes(label = paste0(country, " ", year))) +
  labs(title = "Different sources on English speakers per country")


```


```{r plot-eng-prop}
###### Plot English speaking population against corruption index
supp %>%
  #filter(wgi_est < 1) %>%
  filter(twitter_users_pc < 0.3) %>%
  ggplot(.,
         aes(x = twitter_users_pc,
             y = eng_prop)) +
  geom_point() +
  geom_label_repel(aes(label = paste0(country, " ", year))) +
  labs(title = "English-speaking share by voice and accountability index")



```

```{r plot-tweet-freq}
###### Plot number of tweets every week
tweets %>%
  filter(country == "Nigeria") %>%
  mutate(week = floor_date(date, unit = "week"),
         month = floor_date(date, unit = "month")
  ) %>%
  group_by(month, country, leader) %>%
  summarise(n = n()) %>%
  ggplot(.,
         aes(x = month,
             y = n)) +
  geom_col() +
  labs(title = "Number of Tweets mentioning Buhari in Lagos, Nigeria",
       subtitle = paste0(nrow(tweets), " tweets from Lagos")
  ) 


```

```{r map-tweets}
###### Map Tweets as points
world1 <- map("world", plot = FALSE, fill = TRUE) %>% sf::st_as_sf()
sf::st_as_sf(map("africa", plot = FALSE, fill = TRUE))


###### Nigeria
nigeria_sf <- world1 %>% filter(ID == "Nigeria")

tweets_n <- nrow(tweets)
tweets_n_w_points <- nrow(tweets[!is.na(tweets$place_coordinates_0),])

ggplot() +
  geom_sf(data = nigeria_sf) +
  coord_sf(xlim = c(8.5, 9.5), ylim = c(7, 7.7), expand = FALSE) +
  geom_point(data = tweets, aes(x = place_coordinates_0, y = place_coordinates_1),
             size = 0.7) +
  labs(title = "Lagos in Nigeria: Tweets which included point spatial data",
       subtitle = paste0(tweets_n_w_points, " of ", tweets_n, " tweets include spatial data")
  ) +
  theme_bw()


```

```{r map-gdl}
###### Plot shapefiles data from GDL
subnat %>%
  filter(country == "Afghanistan") %>%
  #filter(year == 2018) %>%
  ggplot(.) +
  geom_sf(aes(fill = popshare)) +
  geom_text(aes(label = region, x = centroid_X, y = centroid_Y)) +
  facet_wrap(~year)

```

```{r map-gpw}
###### Map spatial data from GPW
nga_shp_big <- nga_shp %>%
  filter(UN_2020_E > 600000)

ggplot() +
  #geom_sf(aes(fill = pop)) +
  #geom_sf(data = ne) + # boundary lines
  geom_sf(data = gpw_30_ext, alpha = 0.5) + # GPW areas
  geom_text_repel(data = nga_shp_big, aes(label = NAME2, x = CENTROID_X, y = CENTROID_Y)) +
  geom_sf(data = nga_shp_big) + # GPW admin shapefiles
  #coord_sf(xlim = c(2.5, 15), ylim = c(4, 15), expand = FALSE) +
  coord_sf(xlim = c(2.5, 4), ylim = c(6, 8), expand = FALSE) +
  labs(title = "GPW Population Counts",
       subtitle = NULL) +
  theme_bw()


```

